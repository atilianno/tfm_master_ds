---
title: "TFM_E2_Grupo02"
output:
  word_document: default
  pdf_document: default
  html_document: default
date: "2023-06-08"
---

```{r}
library(dplyr)
library(factoextra)
library(ggplot2)
library(cluster)
library(tidyverse)
library(ggcorrplot)
library(writexl)
library(openxlsx)
library(PerformanceAnalytics)
library(corrplot)
library(ggplot2)
library(car)
library(simstudy)
library(data.table)
library(corrr)
#install.packages("dbscan")
library(dbscan)
library(DMwR2)
library(FactoMineR)
library(factoextra)
library(ggthemes)
```

```{r}
#cargamos los datos con las variables que nos interesan
#df1 = data.frame(read.csv2('C:/Users/linfante/OneDrive/Documentos/MasterBigData/TFM/tfm_master_ds/1 - BackEnd/Datos/alimentos v3.csv', sep=','))
#df2 = data.frame(read.csv2('C:/Users/linfante/OneDrive/Documentos/MasterBigData/TFM/tfm_master_ds/1 - BackEnd/Datos/alimentos_vegan.csv', sep=','))
df = data.frame(read.csv2('C:/Users/linfante/OneDrive/Documentos/MasterBigData/TFM/tfm_master_ds/1 - BackEnd/Datos/alimentos v7.csv', sep=','))
```

# Selecciona columnas a investigar y Elimina registros duplicados

```{r}
df <- df[,c("ISO3","PRODUCT_NAME","ENERGY_100G","FAT_100G","CARBOHYDRATES_100G","PROTEINS_100G")]
#df <- df[,c("PRODUCT_NAME","PROTEINS_100G","CARBOHYDRATES_100G","FAT_100G")]
# Elimina registros duplicados
df <- subset(df, !duplicated(df))

```

```{r}
PRODUCT_NAME <- df[,"PRODUCT_NAME"]
ISO3 <- df[,"ISO3"]
 #Convertir a numerico todo
df_nuevo <- df %>%
  mutate(across(where(is.character), type.convert, as.is = TRUE)) %>%
  select_if(is.numeric)
PRODUCT_NAME <- df[,"PRODUCT_NAME"]
df_nuevo <- cbind(ISO3,PRODUCT_NAME,df_nuevo)
```

# Eliminar vectores fila nulos
```{r}
# Sumar las columnas del dataframe
suma_columnas <- rowSums(df_nuevo[,3:ncol(df_nuevo)])

# Agregar la variable de suma al dataframe
df_nuevo <- cbind(df_nuevo, suma = suma_columnas)

df_nuevo <- subset(df_nuevo, suma != 0)
df_nuevo <- df_nuevo[, -which(names(df_nuevo) == "suma")]

cantidad_ceros_por_columna <- colSums(df_nuevo == 0)

print(cantidad_ceros_por_columna)
```
# Aplicar filtros negativos
```{r}
# Crear un vector con las palabras a buscar
palabras_a_eliminar <- c("SALSA DE SOJA","SAUCE SOJA", "SAUCE SOJA","BEBIDA DE SOJA", "YAOURT SOJA","YOGURT DE SOJA","CHOCOLAT","ARROZ Y SOJA","LECHE","MOUSSE","MILK","ACEITE","DESSERT","PAN SOJA","BOISSON SOJA", "GLACE","ML","SAUCE DE SOJA","SAUCE","SALSA","BIBEDA DE SOJA","LAIT SOJA","VIVESOY SOJA","LAIT DE SOJA","YAOURT","POSTRE","MUESLI","YOGUR","BEBIDA","MARGARINA","VINAIGRETTE","DRINK","SAUCE","BATIDO","LAIT","MAYONNAISE","CAFE","VANILLE","NATA","YOGURT","LACTOVISOY","ALIMENTO DE SOYA","ALIMENTO LIQUIDO DE SOYA")

# Crear una función que verifica si alguna de las palabras está presente en el texto
verificar_palabras <- function(texto, palabras) {
  sapply(palabras, function(palabra) grepl(palabra, texto))
}
# Identificar las filas que contienen alguna de las palabras a eliminar
filas_a_eliminar <- apply(df_nuevo, 1, function(row) any(verificar_palabras(row["PRODUCT_NAME"], palabras_a_eliminar)))

# Eliminar las filas identificadas del dataframe original
df_nuevo1 <- df_nuevo[!filas_a_eliminar, ]
#df_nuevo2 <- df_nuevo1[,-1]

#Elimino los faltantes
df_nuevo1 <- df_nuevo1[complete.cases(df_nuevo1), ]

```

```{r}
df_soja <- subset(df_nuevo1, grepl("soja", PRODUCT_NAME, ignore.case = TRUE)) ## Registros que contienen la palabra "soja"
df_soja <- na.omit(df_soja)

df_seitan <- subset(df_nuevo1, grepl("seitan", PRODUCT_NAME, ignore.case = TRUE)) ## Registros que contienen la palabra "seitan"
df_seitan <- na.omit(df_seitan)

df_tofu <- subset(df_nuevo1, grepl("tofu", PRODUCT_NAME, ignore.case = TRUE)) ## Registros que contienen la palabra "tofu"
df_tofu <- na.omit(df_tofu)

```

# Frecuencia de productos por país de orígen
```{r}
print(table(df_seitan$ISO3))
print(table(df_tofu$ISO3))
print(table(df_soja$ISO3))
```
# SEITAN

```{r}
# Aplicar algoritmo Local Outliers Factor para eliminar atípicos

# 1.- First, we need to convert the incorrect type of variables.

seitan_clean <- df_seitan %>%
  mutate(PRODUCT_NAME = as.factor(PRODUCT_NAME))
str(seitan_clean)

seitan_clean <- as.data.frame(scale(df_seitan[,-1:-2]))
seitan_lof <- lof(seitan_clean, minPts = 7)
head(seitan_clean)

#PCA and use the first two dimensions of the PCA
library(FactoMineR)
library(factoextra)

seitan_pca <- PCA(seitan_clean, scale.unit = F, ncp = 6, graph = F)

```
```{r}
summary(seitan_pca)
```
```{r}
fviz_eig(seitan_pca, ncp = 6, addlabels = T, main = "Variance explained by each dimensions - Seitan")

```
```{r}
library(ggthemes)
seitan_a <- data.frame(seitan_pca$ind$coord[,1:3])
seitan_b <- cbind(seitan_a, lof_score = seitan_lof)
#seitan_b <- cbind(seitan_a, fraud = seitan_clean$, lof_score = seitan_clean$lof)

seitan_lof_visual <- ggplot(seitan_b, aes(x=Dim.1 ,y=Dim.2)) + 
    geom_point(aes(size=lof_score)) +
  ggtitle("LOF Score Distribution - Seitan")+
    theme_minimal()

seitan_lof_visual

```
```{r}
summary(seitan_b)
```
```{r}
seitan_b %>%
  filter(lof_score <= 1.75) %>% 
  ggplot( aes(x=lof_score)) +
    geom_density( color="#e9ecef", fill = "#c90076", alpha=0.7) +
    scale_fill_manual(values="#8fce00") +
    xlab("LOF Score")+
  ggtitle("LOF Score Distribution Seitan")+
    theme_minimal() +
    labs(fill="")
```
```{r}
quantile(seitan_b$lof_score, probs = c(0, 0.8))
```

```{r}
seitan_b <- seitan_b %>% 
  mutate(outlier = ifelse(lof_score > 1.3319969, 1, 0))

seitan_lof_visual_b <- ggplot(seitan_b, aes(x=Dim.1 ,y=Dim.2, color=outlier)) + 
    geom_point() +
  ggtitle("LOF Score Distribution Seitan")+
    theme_minimal()

seitan_lof_visual_b
```
```{r}
outliers <- seitan_b[seitan_b$outlier==1,]
nooutliers <- seitan_b[seitan_b$outlier==0,]
```

```{r}
df_seitan <- df_seitan[seitan_lof < 1.3319969,]
df_seitan_outliers <- df_seitan[seitan_lof >= 1.3319969,]
```

# Eliminar valores atipicos univariantes

```{r}
remove_outliers <- function(data, column, sd_threshold = 2) {
  data[abs(scale(data[[column]])) < sd_threshold, ]
}

columns_to_check <- 3:ncol(df_seitan)
for (column in columns_to_check) {
  df_seitan <- remove_outliers(df_seitan, column)
}
```


```{r}
# Obtener las columnas cuantitativas del dataframe
columnas_cuantitativas <- sapply(df_seitan, is.numeric)

# Crear un histograma para cada columna cuantitativa
for (columna in names(df_seitan[columnas_cuantitativas])) {
  plot_data <- df_seitan[, columna]
  p <- ggplot(data.frame(x = plot_data), aes(x)) +
    geom_histogram(binwidth = 0.5, fill = "steelblue", color = "white") +
    labs(title = paste("Histograma de", columna,"- Seitan"),
         x = columna,
         y = "Frecuencia")
  
  print(p)
}
```
```{r}
summary(as.data.frame(scale(df_seitan[,-1:-2])))
```
#Datos normalizados para el Seitán

```{r}
s_seitan <- as.data.frame(scale(df_seitan[,-1:-2]))
# Obtener las columnas cuantitativas del dataframe
columnas_cuantitativas <- sapply(s_seitan, is.numeric)

# Crear un histograma para cada columna cuantitativa
for (columna in names(s_seitan[columnas_cuantitativas])) {
  plot_data <- s_seitan[, columna]
  p <- ggplot(data.frame(x = plot_data), aes(x)) +
    geom_histogram(binwidth = 0.5, fill = "steelblue", color = "white") +
    labs(title = paste("Histograma de", columna,"- Seitan"),
         x = columna,
         y = "Frecuencia")
  
  print(p)
}
```

```{r}
s_seitan <- scale(df_seitan[,-1:-2])

# total de cluster óptimos
elbow <- fviz_nbclust(x = s_seitan, FUNcluster = kmeans, method = "wss", k.max = 15, 
             diss = get_dist(s_seitan, method = "euclidean"), nstart = 25)
print(elbow)

# set.seed(123)
# km_clusters <- kmeans(x=s_seitan,centers=4,nstart=50)
# fviz_cluster(object=km_clusters,data=s_seitan,show.clust.cent = TRUE,
#              ellipse.type="euclid",star.plot=TRUE,repel=TRUE,
#              pointsize=0.5,outlier.color="darkred") +
#   labs(title ="Resultados clustering K-means") +
#   theme_bw() +
#   theme(legend.position = "none")

set.seed(123)
km_clusters <- kmeans(x = s_seitan, centers = 4, nstart = 50)

fviz_cluster(object = km_clusters, data = s_seitan, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE,
             pointsize = 0.5, outlier.color = "darkred", geom = "point") +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Resultados clustering K-means Seitan")

set.seed(101)

hc_euclidea_av <- hclust(d = dist(x = s_seitan, method = "euclidean"),
                         method = "average")
fviz_dend(x = hc_euclidea_av, k = 4, cex = 0.5,
          k_colors = c("red","blue","green","magenta"),color_labels_by_k = T,
          lwd = 0.2,type = "r",label_cols = rainbow(nrow(df_seitan)),
          rect_lty = "lightblue") +
  geom_hline(yintercept = 3.65, linetype = "dashed") +
  labs(title = "Herarchical clustering Seitan",
       subtitle = "Distancia euclidea, Average, k=4")

pam.res <- pam(s_seitan, 4)
# Visualización
fviz_cluster(pam.res, geom = "point", ellipse.type = "norm",
             show.clust.cent = TRUE,star.plot = TRUE)+
  labs(title = "Resultados clustering K-means superpuestos - Seitan")+ theme_bw()
```

# Biplot PCA y K-Means para medir representatividad seitan

```{r}
# PCA
pca <- prcomp(df_seitan[,-1:-2], scale=TRUE)
df_seitan.pca <- pca$x
# Cluster over the three first PCA dimensions
kc <- kmeans(df_seitan.pca[,1:3], 4)
fviz_pca_biplot(pca, label="var", habillage=as.factor(kc$cluster)) +
  labs(color=NULL) + ggtitle("") +
  theme(text = element_text(size = 15),
        panel.background = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(colour = "black"),
        legend.key = element_rect(fill = "white"))
```
```{r}
library(ggplot2)

# Obtener los centroides de cada clúster
centroids <- as.data.frame(km_clusters$centers)


# Obtener el valor mínimo y máximo en el dataframe
min_value <- min(centroids, na.rm = TRUE)
max_value <- max(centroids, na.rm = TRUE)

# Escalar los datos entre 0 y 1
scaled_df <- (centroids - min_value) / (max_value - min_value)

library(fmsb)


# Define the variable ranges: maximum and minimum
max_min <- data.frame(
  ENERGY_100G = c(1, 0), FAT_100G = c(1, 0), CARBOHYDRATES_100G = c(1, 0),PROTEINS_100G = c(1, 0)
)
rownames(max_min) <- c("Max", "Min")

# Bind the variable ranges to the data
df <- rbind(max_min, scaled_df)
```

```{r}
create_beautiful_radarchart <- function(data, color = "#00AFBB", 
                                        vlabels = colnames(data), vlcex = 0.7,
                                        caxislabels = NULL, title = NULL, ...){
  radarchart(
    data, axistype = 1,
    # Customize the polygon
    pcol = color, pfcol = scales::alpha(color, 0.3), plwd = 2, plty = 1,
    # Customize the grid
    cglcol = "grey", cglty = 1, cglwd = 0.8,
    # Customize the axis
    axislabcol = "grey", 
    # Variable labels
    vlcex = vlcex, vlabels = vlabels,
    caxislabels = caxislabels, title = title, ...
  )
}
```

```{r}
# Reduce plot margin using par()
op <- par(mar = c(1, 2, 2, 1))
create_beautiful_radarchart(df, caxislabels = c(0, .25, .5, .75, 1))
par(op)
```
```{r}
# Reduce plot margin using par()
op <- par(mar = c(1, 2, 2, 2))
# Create the radar charts
create_beautiful_radarchart(
  data = df, caxislabels = c(0, .25, .5, .75, 1),
  color = c("#00AFBB", "#E7B800", "#FC4E07","#7A378B")
)
# Add an horizontal legend
legend(
  x = "bottom", legend = rownames(df[-c(1,2),]), horiz = TRUE,
  bty = "n", pch = 20 , col = c("#00AFBB", "#E7B800", "#FC4E07","#7A378B"),
  text.col = "black", cex = 1, pt.cex = 1.5
  )
par(op)
```


```{r}
# Define colors and titles
colors <- c("#00AFBB", "#E7B800", "#FC4E07","blue")
titles <- c("1", "2", "3","4")

# Reduce plot margin using par()
# Split the screen in 3 parts
op <- par(mar = c(1, 1, 1, 1))

par(mfrow = c(1,4))

# Create the radar chart
for(i in 1:4){
  create_beautiful_radarchart(
    data = df[c(1, 2, i+2), ], caxislabels = c(0, .25, .5, .75, 1),
    color = colors[i], title = titles[i]
    )
}
par(op)

```
# Visualizar los productos más cercanos al centroide que representan cada cluster top10
```{r}
# Realizar clustering en el dataframe
set.seed(123)
df_top <- df_seitan
km_clusters <- kmeans(x = df_top[, c("ENERGY_100G", "FAT_100G", "CARBOHYDRATES_100G", "PROTEINS_100G")], centers = 4, nstart = 50)

# Obtener las asignaciones de clúster
cluster_assignments <- km_clusters$cluster

# Agregar las asignaciones de clúster al dataframe
df_top$cluster <- cluster_assignments

# Inicializar una lista para almacenar los productos más cercanos a cada centroide
closest_products <- vector("list", max(cluster_assignments))

# Encontrar los productos más cercanos a cada centroide
for (cluster in 1:max(cluster_assignments)) {
  cluster_center <- km_clusters$centers[cluster, ]
  distances <- apply(df_top[, c("ENERGY_100G", "FAT_100G", "CARBOHYDRATES_100G", "PROTEINS_100G")], 1, function(row) {
    sum((row - cluster_center)^2)
  })
  closest_products[[cluster]] <- head(order(distances), 10)
}

# Imprimir los productos más cercanos a cada centroide
for (cluster in 1:max(cluster_assignments)) {
  cat("Cluster", cluster, ":\n")
  print(df_top[closest_products[[cluster]], c("PRODUCT_NAME", "ENERGY_100G", "FAT_100G", "CARBOHYDRATES_100G", "PROTEINS_100G")])
  cat("\n")
}
```

##### TOfu

```{r}
# Aplicar algoritmo Local Outliers Factor para eliminar atípicos

# 1.- First, we need to convert the incorrect type of variables.

tofu_clean <- df_tofu %>%
  mutate(PRODUCT_NAME = as.factor(PRODUCT_NAME))
str(tofu_clean)

tofu_clean <- as.data.frame(scale(df_tofu[,-1:-2]))
tofu_lof <- lof(tofu_clean, minPts = 7)
head(tofu_clean)

#PCA and use the first two dimensions of the PCA
library(FactoMineR)
library(factoextra)

tofu_pca <- PCA(tofu_clean, scale.unit = F, ncp = 6, graph = F)
```

```{r}
summary(tofu_pca)
```

```{r}
fviz_eig(tofu_pca, ncp = 6, addlabels = T, main = "Variance explained by each dimensions TOfu")

```
```{r}
library(ggthemes)
tofu_a <- data.frame(tofu_pca$ind$coord[,1:3])
tofu_b <- cbind(tofu_a, lof_score = tofu_lof)
#tofu_b <- cbind(tofu_a, fraud = tofu_clean$, lof_score = tofu_clean$lof)

tofu_lof_visual <- ggplot(tofu_b, aes(x=Dim.1 ,y=Dim.2)) + 
    geom_point(aes(size=lof_score)) +
  ggtitle("LOF Score Distribution Tofu")+
    theme_minimal()

tofu_lof_visual

```
```{r}
summary(tofu_b)

```
```{r}
tofu_b %>%
  filter(lof_score <= 1.75) %>% 
  ggplot( aes(x=lof_score)) +
    geom_density( color="#e9ecef", fill = "#c90076", alpha=0.7) +
    scale_fill_manual(values="#8fce00") +
    xlab("LOF Score")+
  ggtitle("LOF Score Distribution Tofu")+
    theme_minimal() +
    labs(fill="")
```
```{r}
quantile(tofu_b$lof_score, probs = c(0, 0.8))

```

```{r}
tofu_b <- tofu_b %>% 
  mutate(outlier = ifelse(lof_score > 1.2822731, 1, 0))

tofu_lof_visual_b <- ggplot(tofu_b, aes(x=Dim.1 ,y=Dim.2, color=outlier)) + 
    geom_point() +
  ggtitle("LOF Score Distribution Tofu")+
    theme_minimal()

tofu_lof_visual_b
```
```{r}
outliers <- tofu_b[tofu_b$outlier==1,]
nooutliers <- tofu_b[tofu_b$outlier==0,]
```

```{r}
df_tofu <- df_tofu[tofu_lof < 1.2822731,]
df_tofu_outliers <- df_tofu[tofu_lof >= 1.2822731,]

```

# Eliminar valores atipicos univariantes

```{r}
remove_outliers <- function(data, column, sd_threshold = 2) {
  data[abs(scale(data[[column]])) < sd_threshold, ]
}

columns_to_check <- 3:ncol(df_tofu)
for (column in columns_to_check) {
  df_tofu <- remove_outliers(df_tofu, column)
}
```


```{r}
# Obtener las columnas cuantitativas del dataframe
columnas_cuantitativas <- sapply(df_tofu, is.numeric)

# Crear un histograma para cada columna cuantitativa
for (columna in names(df_tofu[columnas_cuantitativas])) {
  plot_data <- df_tofu[, columna]
  p <- ggplot(data.frame(x = plot_data), aes(x)) +
    geom_histogram(binwidth = 0.5, fill = "steelblue", color = "white") +
    labs(title = paste("Histograma de", columna,"- Tofu"),
         x = columna,
         y = "Frecuencia")
  
  print(p)
}
```
```{r}
summary(as.data.frame(scale(df_tofu[,-1:-2])))
```

#Datos normalizados para el tofu

```{r}
s_tofu <- as.data.frame(scale(df_tofu[,-1:-2]))
# Obtener las columnas cuantitativas del dataframe
columnas_cuantitativas <- sapply(s_tofu, is.numeric)

# Crear un histograma para cada columna cuantitativa
for (columna in names(s_tofu[columnas_cuantitativas])) {
  plot_data <- s_tofu[, columna]
  p <- ggplot(data.frame(x = plot_data), aes(x)) +
    geom_histogram(binwidth = 0.5, fill = "steelblue", color = "white") +
    labs(title = paste("Histograma de", columna,"para el Tofu"),
         x = columna,
         y = "Frecuencia")
  
  print(p)
}
```

```{r}
s_tofu <- scale(df_tofu[,-1:-2])

# total de cluster óptimos
elbow <- fviz_nbclust(x = s_tofu, FUNcluster = kmeans, method = "wss", k.max = 15, 
             diss = get_dist(s_tofu, method = "euclidean"), nstart = 25)
print(elbow)

# set.seed(123)
# km_clusters <- kmeans(x=s_tofu,centers=4,nstart=25)
# fviz_cluster(object=km_clusters,data=s_tofu,show.clust.cent = TRUE,
#              ellipse.type="euclid",star.plot=TRUE,repel=TRUE,
#              pointsize=0.5,outlier.color="darkred") +
#   labs(title ="Resultados clustering K-means") +
#   theme_bw() +
#   theme(legend.position = "none")

set.seed(123)
km_clusters <- kmeans(x = s_tofu, centers = 4, nstart = 25)

fviz_cluster(object = km_clusters, data = s_tofu, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE,
             pointsize = 0.5, outlier.color = "darkred", geom = "point") +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Resultados clustering K-means Tofu")

set.seed(101)
hc_euclidea_av <- hclust(d = dist(x = s_tofu, method = "euclidean"),
                         method = "average")
fviz_dend(x = hc_euclidea_av, k = 4, cex = 0.5,
          k_colors = c("red","blue","green","magenta"),color_labels_by_k = T,
          lwd = 0.2,type = "r",label_cols = rainbow(nrow(df_tofu)),
          rect_lty = "lightblue") +
  geom_hline(yintercept = 3.65, linetype = "dashed") +
  labs(title = "Herarchical clustering Tofu",
       subtitle = "Distancia euclidea, Average, k=4")

pam.res <- pam(s_tofu, 4)
# Visualización
fviz_cluster(pam.res, geom = "point", ellipse.type = "norm",
             show.clust.cent = TRUE,star.plot = TRUE)+
  labs(title = "Resultados clustering K-means Superpuestos Tofu")+ theme_bw()
```
# Biplot PCA y K-Means para medir representatividad tofu

```{r}

# PCA
pca <- prcomp(df_tofu[,-1:-2], scale=TRUE)
df_tofu.pca <- pca$x
# Cluster over the three first PCA dimensions
kc <- kmeans(df_tofu.pca[,1:3], 4)
fviz_pca_biplot(pca, label="var", habillage=as.factor(kc$cluster)) +
  labs(color=NULL) + ggtitle("") +
  theme(text = element_text(size = 15),
        panel.background = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(colour = "black"),
        legend.key = element_rect(fill = "white"))
```

```{r}
library(ggplot2)

# Obtener los centroides de cada clúster
centroids <- as.data.frame(km_clusters$centers)


# Obtener el valor mínimo y máximo en el dataframe
min_value <- min(centroids, na.rm = TRUE)
max_value <- max(centroids, na.rm = TRUE)

# Escalar los datos entre 0 y 1
scaled_df <- (centroids - min_value) / (max_value - min_value)

library(fmsb)


# Define the variable ranges: maximum and minimum
max_min <- data.frame(
  ENERGY_100G = c(1, 0), FAT_100G = c(1, 0), CARBOHYDRATES_100G = c(1, 0),PROTEINS_100G = c(1, 0)
)
rownames(max_min) <- c("Max", "Min")

# Bind the variable ranges to the data
df <- rbind(max_min, scaled_df)
```

```{r}
create_beautiful_radarchart <- function(data, color = "#00AFBB", 
                                        vlabels = colnames(data), vlcex = 0.7,
                                        caxislabels = NULL, title = NULL, ...){
  radarchart(
    data, axistype = 1,
    # Customize the polygon
    pcol = color, pfcol = scales::alpha(color, 0.3), plwd = 2, plty = 1,
    # Customize the grid
    cglcol = "grey", cglty = 1, cglwd = 0.8,
    # Customize the axis
    axislabcol = "grey", 
    # Variable labels
    vlcex = vlcex, vlabels = vlabels,
    caxislabels = caxislabels, title = title, ...
  )
}
```

```{r}
# Reduce plot margin using par()
op <- par(mar = c(1, 2, 2, 1))
create_beautiful_radarchart(df, caxislabels = c(0, .25, .5, .75, 1))
par(op)
```
```{r}
# Reduce plot margin using par()
op <- par(mar = c(1, 2, 2, 2))
# Create the radar charts
create_beautiful_radarchart(
  data = df, caxislabels = c(0, .25, .5, .75, 1),
  color = c("#00AFBB", "#E7B800", "#FC4E07","#7A378B")
)
# Add an horizontal legend
legend(
  x = "bottom", legend = rownames(df[-c(1,2),]), horiz = TRUE,
  bty = "n", pch = 20 , col = c("#00AFBB", "#E7B800", "#FC4E07","#7A378B"),
  text.col = "black", cex = 1, pt.cex = 1.5
  )
par(op)
```


```{r}
# Define colors and titles
colors <- c("#00AFBB", "#E7B800", "#FC4E07","blue")
titles <- c("1", "2", "3","4")

# Reduce plot margin using par()
# Split the screen in 3 parts
op <- par(mar = c(1, 1, 1, 1))

par(mfrow = c(1,4))

# Create the radar chart
for(i in 1:4){
  create_beautiful_radarchart(
    data = df[c(1, 2, i+2), ], caxislabels = c(0, .25, .5, .75, 1),
    color = colors[i], title = titles[i]
    )
}
par(op)

```
# Visualizar los productos más cercanos al centroide que representan cada cluster top10
```{r}
# Realizar clustering en el dataframe
set.seed(123)
df_top <- df_tofu
km_clusters <- kmeans(x = df_top[, c("ENERGY_100G", "FAT_100G", "CARBOHYDRATES_100G", "PROTEINS_100G")], centers = 4, nstart = 50)

# Obtener las asignaciones de clúster
cluster_assignments <- km_clusters$cluster

# Agregar las asignaciones de clúster al dataframe
df_top$cluster <- cluster_assignments

# Inicializar una lista para almacenar los productos más cercanos a cada centroide
closest_products <- vector("list", max(cluster_assignments))

# Encontrar los productos más cercanos a cada centroide
for (cluster in 1:max(cluster_assignments)) {
  cluster_center <- km_clusters$centers[cluster, ]
  distances <- apply(df_top[, c("ENERGY_100G", "FAT_100G", "CARBOHYDRATES_100G", "PROTEINS_100G")], 1, function(row) {
    sum((row - cluster_center)^2)
  })
  closest_products[[cluster]] <- head(order(distances), 10)
}

# Imprimir los productos más cercanos a cada centroide
for (cluster in 1:max(cluster_assignments)) {
  cat("Cluster", cluster, ":\n")
  print(df_top[closest_products[[cluster]], c("PRODUCT_NAME", "ENERGY_100G", "FAT_100G", "CARBOHYDRATES_100G", "PROTEINS_100G")])
  cat("\n")
}
```


#### SOja



```{r}
# Aplicar algoritmo Local Outliers Factor para eliminar atípicos

# 1.- First, we need to convert the incorrect type of variables.

soja_clean <- df_soja %>%
  mutate(PRODUCT_NAME = as.factor(PRODUCT_NAME))
str(soja_clean)

soja_clean <- as.data.frame(scale(df_soja[,-1:-2]))
soja_lof <- lof(soja_clean, minPts = 7)
head(soja_clean)

#PCA and use the first two dimensions of the PCA
library(FactoMineR)
library(factoextra)

soja_pca <- PCA(soja_clean, scale.unit = F, ncp = 6, graph = F)
```

```{r}
summary(soja_pca)

```
```{r}
fviz_eig(soja_pca, ncp = 6, addlabels = T, main = "Variance explained by each dimensions - Soja")

```
```{r}
library(ggthemes)
soja_a <- data.frame(soja_pca$ind$coord[,1:3])
soja_b <- cbind(soja_a, lof_score = soja_lof)
#soja_b <- cbind(soja_a, fraud = soja_clean$, lof_score = soja_clean$lof)

soja_lof_visual <- ggplot(soja_b, aes(x=Dim.1 ,y=Dim.2)) + 
    geom_point(aes(size=lof_score)) +
  ggtitle("LOF Score Distribution - Soja")+
    theme_minimal()

soja_lof_visual

```
```{r}
summary(soja_b)

```
```{r}
soja_b %>%
  filter(lof_score <= 1.75) %>% 
  ggplot( aes(x=lof_score)) +
    geom_density( color="#e9ecef", fill = "#c90076", alpha=0.7) +
    scale_fill_manual(values="#8fce00") +
    xlab("LOF Score")+
  ggtitle("LOF Score Distribution - Soja")+
    theme_minimal() +
    labs(fill="")
```
```{r}
quantile(soja_b$lof_score, probs = c(0, 0.8))

```

```{r}
soja_b <- soja_b %>% 
  mutate(outlier = ifelse(lof_score > 1.4686290, 1, 0))

soja_lof_visual_b <- ggplot(soja_b, aes(x=Dim.1 ,y=Dim.2, color=outlier)) + 
    geom_point() +
  ggtitle("LOF Score Distribution - Soja")+
    theme_minimal()

soja_lof_visual_b
```
```{r}
outliers <- soja_b[soja_b$outlier==1,]
nooutliers <- soja_b[soja_b$outlier==0,]
```

```{r}
df_soja <- df_soja[soja_lof < 1.4686290,]
df_soja_outliers <- df_soja[soja_lof >= 1.4686290,]

```

# Eliminar valores atipicos univariantes

```{r}
remove_outliers <- function(data, column, sd_threshold = 2) {
  data[abs(scale(data[[column]])) < sd_threshold, ]
}

columns_to_check <- 3:ncol(df_soja)
for (column in columns_to_check) {
  df_soja <- remove_outliers(df_soja, column)
}
```


```{r}
# Obtener las columnas cuantitativas del dataframe
columnas_cuantitativas <- sapply(df_soja, is.numeric)

# Crear un histograma para cada columna cuantitativa
for (columna in names(df_soja[columnas_cuantitativas])) {
  plot_data <- df_soja[, columna]
  p <- ggplot(data.frame(x = plot_data), aes(x)) +
    geom_histogram(binwidth = 0.5, fill = "steelblue", color = "white") +
    labs(title = paste("Histograma de", columna,"- Soja"),
         x = columna,
         y = "Frecuencia")
  
  print(p)
}
```
```{r}
summary(as.data.frame(scale(df_soja[,-1:-2])))
```

#Datos normalizados para el soja

```{r}
s_soja <- as.data.frame(scale(df_soja[,-1:-2]))
# Obtener las columnas cuantitativas del dataframe
columnas_cuantitativas <- sapply(s_soja, is.numeric)

# Crear un histograma para cada columna cuantitativa
for (columna in names(s_soja[columnas_cuantitativas])) {
  plot_data <- s_soja[, columna]
  p <- ggplot(data.frame(x = plot_data), aes(x)) +
    geom_histogram(binwidth = 0.5, fill = "steelblue", color = "white") +
    labs(title = paste("Histograma de", columna,"- Soja"),
         x = columna,
         y = "Frecuencia")
  
  print(p)
}
```

```{r}
s_soja <- scale(df_soja[,-1:-2])

# total de cluster óptimos
elbow <- fviz_nbclust(x = s_soja, FUNcluster = kmeans, method = "wss", k.max = 15, 
             diss = get_dist(s_soja, method = "euclidean"), nstart = 25)
print(elbow)

# set.seed(123)
# km_clusters <- kmeans(x=s_soja,centers=4,nstart=25)
# fviz_cluster(object=km_clusters,data=s_soja,show.clust.cent = TRUE,
#              ellipse.type="euclid",star.plot=TRUE,repel=TRUE,
#              pointsize=0.5,outlier.color="darkred") +
#   labs(title ="Resultados clustering K-means") +
#   theme_bw() +
#   theme(legend.position = "none")

set.seed(123)
km_clusters <- kmeans(x = s_soja, centers = 4, nstart = 50)

fviz_cluster(object = km_clusters, data = s_soja, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE,
             pointsize = 0.5, outlier.color = "darkred", geom = "point") +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle("Resultados clustering K-means - Soja")

set.seed(101)
hc_euclidea_av <- hclust(d = dist(x = s_soja, method = "euclidean"),
                         method = "average")
fviz_dend(x = hc_euclidea_av, k = 4, cex = 0.5,
          k_colors = c("red","blue","green","magenta"),color_labels_by_k = T,
          lwd = 0.2,type = "r",label_cols = rainbow(nrow(df_soja)),
          rect_lty = "lightblue") +
  geom_hline(yintercept = 3.65, linetype = "dashed") +
  labs(title = "Herarchical clustering Soja",
       subtitle = "Distancia euclidea, Average, k=4")

pam.res <- pam(s_soja, 4)
# Visualización
fviz_cluster(pam.res, geom = "point", ellipse.type = "norm",
             show.clust.cent = TRUE,star.plot = TRUE)+
  labs(title = "Resultados clustering K-means superpuesto Soja")+ theme_bw()
```
# Biplot PCA y K-Means para medir representatividad soja

```{r}

# PCA
pca <- prcomp(df_soja[,-1:-2], scale=TRUE)
df_soja.pca <- pca$x
# Cluster over the three first PCA dimensions
kc <- kmeans(df_soja.pca[,1:3], 4)
fviz_pca_biplot(pca, label="var", habillage=as.factor(kc$cluster)) +
  labs(color=NULL) + ggtitle("") +
  theme(text = element_text(size = 15),
        panel.background = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(colour = "black"),
        legend.key = element_rect(fill = "white"))
```

```{r}
library(ggplot2)

# Obtener los centroides de cada clúster
centroids <- as.data.frame(km_clusters$centers)


# Obtener el valor mínimo y máximo en el dataframe
min_value <- min(centroids, na.rm = TRUE)
max_value <- max(centroids, na.rm = TRUE)

# Escalar los datos entre 0 y 1
scaled_df <- (centroids - min_value) / (max_value - min_value)

library(fmsb)


# Define the variable ranges: maximum and minimum
max_min <- data.frame(
  ENERGY_100G = c(1, 0), FAT_100G = c(1, 0), CARBOHYDRATES_100G = c(1, 0),PROTEINS_100G = c(1, 0)
)
rownames(max_min) <- c("Max", "Min")

# Bind the variable ranges to the data
df <- rbind(max_min, scaled_df)
```

```{r}
create_beautiful_radarchart <- function(data, color = "#00AFBB", 
                                        vlabels = colnames(data), vlcex = 0.7,
                                        caxislabels = NULL, title = NULL, ...){
  radarchart(
    data, axistype = 1,
    # Customize the polygon
    pcol = color, pfcol = scales::alpha(color, 0.3), plwd = 2, plty = 1,
    # Customize the grid
    cglcol = "grey", cglty = 1, cglwd = 0.8,
    # Customize the axis
    axislabcol = "grey", 
    # Variable labels
    vlcex = vlcex, vlabels = vlabels,
    caxislabels = caxislabels, title = title, ...
  )
}
```

```{r}
# Reduce plot margin using par()
op <- par(mar = c(1, 2, 2, 1))
create_beautiful_radarchart(df, caxislabels = c(0, .25, .5, .75, 1))
par(op)
```
```{r}
# Reduce plot margin using par()
op <- par(mar = c(1, 2, 2, 2))
# Create the radar charts
create_beautiful_radarchart(
  data = df, caxislabels = c(0, .25, .5, .75, 1),
  color = c("#00AFBB", "#E7B800", "#FC4E07","#7A378B")
)
# Add an horizontal legend
legend(
  x = "bottom", legend = rownames(df[-c(1,2),]), horiz = TRUE,
  bty = "n", pch = 20 , col = c("#00AFBB", "#E7B800", "#FC4E07","#7A378B"),
  text.col = "black", cex = 1, pt.cex = 1.5
  )
par(op)
```


```{r}
# Define colors and titles
colors <- c("#00AFBB", "#E7B800", "#FC4E07","blue")
titles <- c("1", "2", "3","4")

# Reduce plot margin using par()
# Split the screen in 3 parts
op <- par(mar = c(1, 1, 1, 1))

par(mfrow = c(1,4))

# Create the radar chart
for(i in 1:4){
  create_beautiful_radarchart(
    data = df[c(1, 2, i+2), ], caxislabels = c(0, .25, .5, .75, 1),
    color = colors[i], title = titles[i]
    )
}
par(op)

```

# Visualizar los productos más cercanos al centroide que representan cada cluster top10
```{r}
# Realizar clustering en el dataframe
set.seed(123)
df_top <- df_soja
km_clusters <- kmeans(x = df_top[, c("ENERGY_100G", "FAT_100G", "CARBOHYDRATES_100G", "PROTEINS_100G")], centers = 4, nstart = 50)

# Obtener las asignaciones de clúster
cluster_assignments <- km_clusters$cluster

# Agregar las asignaciones de clúster al dataframe
df_top$cluster <- cluster_assignments

# Inicializar una lista para almacenar los productos más cercanos a cada centroide
closest_products <- vector("list", max(cluster_assignments))

# Encontrar los productos más cercanos a cada centroide
for (cluster in 1:max(cluster_assignments)) {
  cluster_center <- km_clusters$centers[cluster, ]
  distances <- apply(df_top[, c("ENERGY_100G", "FAT_100G", "CARBOHYDRATES_100G", "PROTEINS_100G")], 1, function(row) {
    sum((row - cluster_center)^2)
  })
  closest_products[[cluster]] <- head(order(distances), 10)
}

# Imprimir los productos más cercanos a cada centroide
for (cluster in 1:max(cluster_assignments)) {
  cat("Cluster", cluster, ":\n")
  print(df_top[closest_products[[cluster]], c("PRODUCT_NAME", "ENERGY_100G", "FAT_100G", "CARBOHYDRATES_100G", "PROTEINS_100G")])
  cat("\n")
}
```


